{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from lib.lib_utils import Utils \n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "import cv2\n",
    "import torch as Torch\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generando .png del file graphene_238951.xyz...\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#trasformazione .xyz to .png \n",
    "Utils.from_xyz_to_png(\n",
    "    Path('/home/gabro/GrapheDetectProject/data.xyz/subset_xyz'), \n",
    "    Path('/home/gabro/GrapheDetectProject/'), \n",
    "    1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divisione del dataset in train/ test/\n",
    "#inserire percorso dataset da dividere e percentuale del test \n",
    "Utils.split_dataset('/home/gabro/GrapheDetectProject/data_yolo_new/', 0.2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "# model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"/home/gabro/GrapheDetectProject/best_100_campioni_new.pt\")  # load a pretrained model (recommended for training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model\n",
    "# model.train(data=\"coco128.yaml\", epochs=3)  # train the model\n",
    "model.train(data=\"config_yolov8.yaml\", epochs=100)  # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.88 ðŸš€ Python-3.11.3 torch-2.0.0+cu117 CPU\n",
      "YOLOv8n summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "'/content/gdrive/MyDrive/Materiale tirocinio Gabriele/GrapheDefectDetector/google_colab_config.yaml' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mval()  \u001b[39m# evaluate model performance on the validation set\u001b[39;00m\n\u001b[1;32m      2\u001b[0m results \u001b[39m=\u001b[39m model(\u001b[39m\"\u001b[39m\u001b[39m/home/gabro/GrapheDetectProject/b74c9ce2-graphene_218641_bonds.png\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# predict on an image\u001b[39;00m\n\u001b[1;32m      3\u001b[0m success \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mexport(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monnx\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# export the model to ONNX format\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/ultralytics/yolo/engine/model.py:301\u001b[0m, in \u001b[0;36mYOLO.val\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m args\u001b[39m.\u001b[39mimgsz \u001b[39m=\u001b[39m check_imgsz(args\u001b[39m.\u001b[39mimgsz, max_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    300\u001b[0m validator \u001b[39m=\u001b[39m TASK_MAP[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask][\u001b[39m2\u001b[39m](args\u001b[39m=\u001b[39margs, _callbacks\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[0;32m--> 301\u001b[0m validator(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39m=\u001b[39m validator\u001b[39m.\u001b[39mmetrics\n\u001b[1;32m    304\u001b[0m \u001b[39mreturn\u001b[39;00m validator\u001b[39m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/ultralytics/yolo/engine/validator.py:125\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m    122\u001b[0m         LOGGER\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mForcing batch=1 square inference (1,3,\u001b[39m\u001b[39m{\u001b[39;00mimgsz\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mimgsz\u001b[39m}\u001b[39;00m\u001b[39m) for non-PyTorch models\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdata, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.yaml\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m check_det_dataset(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m    126\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mclassify\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m check_cls_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/ultralytics/yolo/data/utils.py:195\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[0;34m(dataset, autodownload)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_det_dataset\u001b[39m(dataset, autodownload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    194\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Download, check and/or unzip dataset if not found locally.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     data \u001b[39m=\u001b[39m check_file(dataset)\n\u001b[1;32m    197\u001b[0m     \u001b[39m# Download (optional)\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     extract_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/ultralytics/yolo/utils/checks.py:292\u001b[0m, in \u001b[0;36mcheck_file\u001b[0;34m(file, suffix, download, hard)\u001b[0m\n\u001b[1;32m    290\u001b[0m     files\u001b[39m.\u001b[39mextend(glob\u001b[39m.\u001b[39mglob(\u001b[39mstr\u001b[39m(ROOT \u001b[39m/\u001b[39m d \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39m**\u001b[39m\u001b[39m'\u001b[39m \u001b[39m/\u001b[39m file), recursive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))  \u001b[39m# find file\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m files \u001b[39mand\u001b[39;00m hard:\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(files) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m hard:\n\u001b[1;32m    294\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMultiple files match \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, specify exact path: \u001b[39m\u001b[39m{\u001b[39;00mfiles\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: '/content/gdrive/MyDrive/Materiale tirocinio Gabriele/GrapheDefectDetector/google_colab_config.yaml' does not exist"
     ]
    }
   ],
   "source": [
    "metrics = model.val()  # evaluate model performance on the validation set\n",
    "results = model(\"/home/gabro/GrapheDetectProject/b74c9ce2-graphene_218641_bonds.png\")  # predict on an image\n",
    "success = model.export(format=\"onnx\")  # export the model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Utils' has no attribute 'from_crops_to_thresh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m Utils\u001b[39m.\u001b[39mfunzione_prova()\n\u001b[1;32m      2\u001b[0m \u001b[39m# Utils.crop_from_folder(\"/home/gabro/GrapheDetectProject/immProvaInferenza\", \"/home/gabro/GrapheDetectProject/cartellaCrop\", model)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m Utils\u001b[39m.\u001b[39;49mfrom_crops_to_thresh(\u001b[39m'\u001b[39m\u001b[39m/home/gabro/GrapheDetectProject/cartellaCrop/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Utils' has no attribute 'from_crops_to_thresh'"
     ]
    }
   ],
   "source": [
    "Utils.funzione_prova()\n",
    "Utils.crop_from_folder(\"/home/gabro/GrapheDetectProject/immProvaInferenza\", \"/home/gabro/GrapheDetectProject/cartellaCrop\", model)\n",
    "Utils.from_crops_to_thresh('/home/gabro/GrapheDetectProject/cartellaCrop/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferenza e trasformazione boxes in immagini singole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/gabro/GrapheDetectProject/b74c9ce2-graphene_218641_bonds.png: 640x640 2 defects, 278.0ms\n",
      "Speed: 2.5ms preprocess, 278.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fac5848e530>\n",
      "tensor([[ 4.3943, 18.4342, 34.3378, 50.7019]])\n",
      "(69, 70, 3)\n",
      "tensor([[37.1617, 33.1211, 59.6014, 56.4856]])\n",
      "(69, 70, 3)\n"
     ]
    }
   ],
   "source": [
    "#crop su immagine singola\n",
    "pathImmProva = \"/home/gabro/GrapheDetectProject/b74c9ce2-graphene_218641_bonds.png\"\n",
    "results = model(pathImmProva)  # predict on an image\n",
    "\n",
    "boxes = results[0].boxes\n",
    "index = 0\n",
    "print(boxes.data.size)\n",
    "# for box in boxes.data:\n",
    "#     print(box.xyxy)\n",
    "for box in boxes:\n",
    "    #devo trasformare xy (alto sx), xy (alto dx) in [start_row:end_row, start_col:end_col]\n",
    "    print(boxes[index].xyxy)    #posizione angolo in alto a sx e in basso a dx in pixel\n",
    "    array = Torch.Tensor.numpy(boxes[index].xyxy)\n",
    "    # print(array.size)\n",
    "    x1 = math.ceil(array[0,0])\n",
    "    y1 = math.ceil(array[0,1])\n",
    "    x2 = math.floor(array[0,2])\n",
    "    y2 = math.floor(array[0,3])\n",
    "\n",
    "    # res_plotted = results[0].plot(labels = False, line_width = 1)\n",
    "    # plt.imshow(res_plotted)\n",
    "    # plt.show()\n",
    "    # cv2.imshow(\"result\", res_plotted)\n",
    "\n",
    "    #crop difetto\n",
    "    img = cv2.imread(pathImmProva)\n",
    "    print(img.shape) # Print image shape\n",
    "    # cv2.imshow(\"original\", img)\n",
    "    \n",
    "    # Cropping an image\n",
    "    cropped_image = img[y1:y2, x1:x2] #img[start_row:end_row, start_col:end_col]\n",
    "\n",
    "    # Display cropped image\n",
    "    # cv2.imshow(\"cropped\", cropped_image)\n",
    "\n",
    "    nameCropped = pathImmProva.removesuffix('.png') + \"_cropped_box_\" + str(index)+ \".png\"\n",
    "    # Save the cropped image\n",
    "    cv2.imwrite(nameCropped, cropped_image)\n",
    "    \n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    index = index+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/gabro/GrapheDetectProject/immProvaInferenza/0e9ff1ff-graphene_176095_bonds.png: 640x640 2 defects, 546.3ms\n",
      "Speed: 5.5ms preprocess, 546.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7f63f9812080>\n",
      "tensor([[38.5175, 39.5588, 51.7656, 52.2145]])\n",
      "(69, 70, 3)\n",
      "tensor([[ 3.4684, 16.6030, 41.2881, 53.1532]])\n",
      "(69, 70, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/gabro/GrapheDetectProject/immProvaInferenza/0a288a23-graphene_227444_bonds.png: 640x640 2 defects, 541.7ms\n",
      "Speed: 1.4ms preprocess, 541.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7f63f96b8050>\n",
      "tensor([[22.3563, 30.9228, 51.8132, 53.0598]])\n",
      "(69, 70, 3)\n",
      "tensor([[ 0.0000, 18.6053, 17.3393, 35.6328]])\n",
      "(69, 70, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/gabro/GrapheDetectProject/immProvaInferenza/2d81c496-graphene_122634_bonds.png: 640x640 2 defects, 383.3ms\n",
      "Speed: 3.7ms preprocess, 383.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7f63f96bb070>\n",
      "tensor([[ 6.4632, 15.2746, 62.6588, 67.0441]])\n",
      "(69, 70, 3)\n",
      "tensor([[16.2314, 14.1654, 70.0000, 61.4681]])\n",
      "(69, 70, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/gabro/GrapheDetectProject/immProvaInferenza/0a5c3409-graphene_301236_bonds.png: 640x640 1 defect, 430.8ms\n",
      "Speed: 5.3ms preprocess, 430.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7f63f98620d0>\n",
      "tensor([[34.9728,  0.5022, 49.4850, 13.9389]])\n",
      "(69, 70, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/gabro/GrapheDetectProject/immProvaInferenza/2f01675e-graphene_231617_bonds.png: 640x640 1 defect, 385.5ms\n",
      "Speed: 5.3ms preprocess, 385.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7f63f96bb340>\n",
      "tensor([[25.0373, 23.2366, 56.0320, 60.3772]])\n",
      "(69, 70, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/gabro/GrapheDetectProject/immProvaInferenza/2c60508d-graphene_312780_bonds.png: 640x640 1 defect, 421.3ms\n",
      "Speed: 4.9ms preprocess, 421.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7f63f96e3d40>\n",
      "tensor([[21.1721, 36.0090, 54.6227, 68.6001]])\n",
      "(69, 70, 3)\n"
     ]
    }
   ],
   "source": [
    "#crop_from_folder\n",
    "#prende una cartella di immagini su cui fare detection dei difetti,\n",
    "#deposita su una cartella i difetti croppati \n",
    "\n",
    "#versione con for al posto di .predict\n",
    "cartellaImm = \"/home/gabro/GrapheDetectProject/immProvaInferenza/\"\n",
    "\n",
    "cartellaCrop = \"/home/gabro/GrapheDetectProject/cartellaCrop/\"  #cartella dove andranno messi i difetti croppati \n",
    "\n",
    "\n",
    "for nomeImm in os.listdir(cartellaImm): #ietro i nomi delle immagini all'interno della cartella\n",
    "\n",
    "    pathImm = os.path.join(cartellaImm, nomeImm)    #estraggo il path dell'immagine \n",
    "    results = model(pathImm)  # predict on an image\n",
    "    boxes = results[0].boxes\n",
    "    index = 0\n",
    "    print(boxes.data.size)\n",
    "    # for box in boxes.data:\n",
    "    #     print(box.xyxy)\n",
    "    for box in boxes:\n",
    "        #devo trasformare xy (alto sx), xy (alto dx) in [start_row:end_row, start_col:end_col]\n",
    "        print(boxes[index].xyxy)    #posizione angolo in alto a sx e in basso a dx in pixel\n",
    "        array = Torch.Tensor.numpy(boxes[index].xyxy)\n",
    "        # print(array.size)\n",
    "        x1 = math.floor(array[0,0])\n",
    "        y1 = math.floor(array[0,1])\n",
    "        x2 = math.ceil(array[0,2])\n",
    "        y2 = math.ceil(array[0,3])\n",
    "\n",
    "        # res_plotted = results[0].plot(labels = False, line_width = 1)\n",
    "        # plt.imshow(res_plotted)\n",
    "        # plt.show()\n",
    "        # cv2.imshow(\"result\", res_plotted)\n",
    "\n",
    "        #crop difetto\n",
    "        img = cv2.imread(pathImm)\n",
    "        print(img.shape) # Print image shape\n",
    "        # cv2.imshow(\"original\", img)\n",
    "        \n",
    "        # Cropping an image\n",
    "        cropped_image = img[y1:y2, x1:x2] #img[start_row:end_row, start_col:end_col]\n",
    "\n",
    "        # Display cropped image\n",
    "        # cv2.imshow(\"cropped\", cropped_image)\n",
    "\n",
    "        nomeCropped = os.path.basename(pathImm).removesuffix('.png') + \"_cropped_box_\" + str(index)+ \".png\" #creo nome dell'immagine croppata\n",
    "        pathCropped = os.path.join(cartellaCrop, nomeCropped)   #compongo il path dell'immagine croppata aggiungendo il giusto percorso di output\n",
    "        # Save the cropped image\n",
    "        cv2.imwrite(pathCropped, cropped_image)\n",
    "        \n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        index = index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/6 /home/gabro/GrapheDetectProject/immProvaInferenza/0a288a23-graphene_227444_bonds.png: 640x640 2 defects, 288.6ms\n",
      "image 2/6 /home/gabro/GrapheDetectProject/immProvaInferenza/0a5c3409-graphene_301236_bonds.png: 640x640 1 defect, 236.9ms\n",
      "image 3/6 /home/gabro/GrapheDetectProject/immProvaInferenza/0e9ff1ff-graphene_176095_bonds.png: 640x640 2 defects, 301.1ms\n",
      "image 4/6 /home/gabro/GrapheDetectProject/immProvaInferenza/2c60508d-graphene_312780_bonds.png: 640x640 1 defect, 300.2ms\n",
      "image 5/6 /home/gabro/GrapheDetectProject/immProvaInferenza/2d81c496-graphene_122634_bonds.png: 640x640 2 defects, 302.2ms\n",
      "image 6/6 /home/gabro/GrapheDetectProject/immProvaInferenza/2f01675e-graphene_231617_bonds.png: 640x640 1 defect, 274.1ms\n",
      "Speed: 1.8ms preprocess, 283.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gabro/GrapheDetectProject/immProvaInferenza/0e9ff1ff-graphene_176095_bonds.png\n",
      "<built-in method size of Tensor object at 0x7f24532e2cb0>\n",
      "tensor([[22.3563, 30.9228, 51.8132, 53.0598]])\n",
      "tensor([[ 0.0000, 18.6053, 17.3393, 35.6328]])\n",
      "/home/gabro/GrapheDetectProject/immProvaInferenza/0a288a23-graphene_227444_bonds.png\n",
      "<built-in method size of Tensor object at 0x7f24532e3f70>\n",
      "tensor([[34.9728,  0.5022, 49.4850, 13.9389]])\n",
      "/home/gabro/GrapheDetectProject/immProvaInferenza/2d81c496-graphene_122634_bonds.png\n",
      "<built-in method size of Tensor object at 0x7f24532e1fe0>\n",
      "tensor([[38.5175, 39.5588, 51.7656, 52.2145]])\n",
      "tensor([[ 3.4684, 16.6030, 41.2881, 53.1532]])\n",
      "/home/gabro/GrapheDetectProject/immProvaInferenza/0a5c3409-graphene_301236_bonds.png\n",
      "<built-in method size of Tensor object at 0x7f24532e1d60>\n",
      "tensor([[21.1721, 36.0090, 54.6227, 68.6001]])\n",
      "/home/gabro/GrapheDetectProject/immProvaInferenza/2f01675e-graphene_231617_bonds.png\n",
      "<built-in method size of Tensor object at 0x7f24532e32a0>\n",
      "tensor([[ 6.4632, 15.2746, 62.6588, 67.0441]])\n",
      "tensor([[16.2314, 14.1654, 70.0000, 61.4681]])\n",
      "/home/gabro/GrapheDetectProject/immProvaInferenza/2c60508d-graphene_312780_bonds.png\n",
      "<built-in method size of Tensor object at 0x7f24532e3c00>\n",
      "tensor([[25.0373, 23.2366, 56.0320, 60.3772]])\n"
     ]
    }
   ],
   "source": [
    "#non funziona (usare codice in alto)\n",
    "cartellaImm = \"/home/gabro/GrapheDetectProject/immProvaInferenza\"\n",
    "results = model(cartellaImm, save = False)  # predict on an image\n",
    "\n",
    "for (r, nomeImm) in zip(results, os.listdir(cartellaImm)):\n",
    "    pathImm = os.path.join(cartellaImm, nomeImm)\n",
    "    print(pathImm)\n",
    "\n",
    "    boxes = r.boxes\n",
    "    index = 0\n",
    "    print(boxes.data.size)\n",
    "    # for box in boxes.data:\n",
    "    #     print(box.xyxy)\n",
    "    for box in boxes:\n",
    "        #devo trasformare xy (alto sx), xy (alto dx) in [start_row:end_row, start_col:end_col]\n",
    "        print(box.xyxy)    #posizione angolo in alto a sx e in basso a dx in pixel\n",
    "        array = Torch.Tensor.numpy(box.xyxy)\n",
    "        # print(array.size)\n",
    "        x1 = math.ceil(array[0,0])\n",
    "        y1 = math.ceil(array[0,1])\n",
    "        x2 = math.floor(array[0,2])\n",
    "        y2 = math.floor(array[0,3])\n",
    "\n",
    "        # res_plotted = results[0].plot(labels = False, line_width = 1)\n",
    "        # plt.imshow(res_plotted)\n",
    "        # plt.show()\n",
    "        # cv2.imshow(\"result\", res_plotted)\n",
    "\n",
    "        #crop difetto\n",
    "        img = cv2.imread(pathImm)\n",
    "        # print(img.shape) # Print image shape\n",
    "        # cv2.imshow(\"original\", img)\n",
    "        \n",
    "        # Cropping an image\n",
    "        cropped_image = img[y1:y2, x1:x2] #img[start_row:end_row, start_col:end_col]\n",
    "\n",
    "        # Display cropped image\n",
    "        # cv2.imshow(\"cropped\", cropped_image)\n",
    "\n",
    "        nameCropped = pathImm.removesuffix('.png') + \"_cropped_box_\" + str(index)+ \".png\"\n",
    "        # Save the cropped image\n",
    "        cv2.imwrite(nameCropped, cropped_image)\n",
    "        \n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        index = index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1msave_dir\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['save_crop=False', 'save=True', 'save_period=-1'].\n\n    Arguments received: ['yolo', '--ip=127.0.0.1', '--stdin=9003', '--control=9001', '--hb=9000', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"0345ed87-9ffd-412a-81c1-86603fceaaff\"', '--shell=9002', '--transport=\"tcp\"', '--iopub=9004', '--f=/home/gabro/.local/share/jupyter/runtime/kernel-v2-2901WgmI7MIVlul5.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[14], line 3\u001b[0m\n    results = model.predict(pathCartellaImm, save_crop = True, save_dir='/home/gabro/GrapheDetectProject/dataset_per_analisi_feature_grafiche/crops/')\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m in \u001b[1;35mdecorate_context\u001b[0m\n    return func(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/ultralytics/yolo/engine/model.py:251\u001b[0m in \u001b[1;35mpredict\u001b[0m\n    self.predictor.args = get_cfg(self.predictor.args, overrides)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/ultralytics/yolo/cfg/__init__.py:111\u001b[0m in \u001b[1;35mget_cfg\u001b[0m\n    check_cfg_mismatch(cfg, overrides)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/envs/graphe-env/lib/python3.11/site-packages/ultralytics/yolo/cfg/__init__.py:178\u001b[0;36m in \u001b[0;35mcheck_cfg_mismatch\u001b[0;36m\n\u001b[0;31m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '\u001b[31m\u001b[1msave_dir\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['save_crop=False', 'save=True', 'save_period=-1'].\n\n    Arguments received: ['yolo', '--ip=127.0.0.1', '--stdin=9003', '--control=9001', '--hb=9000', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"0345ed87-9ffd-412a-81c1-86603fceaaff\"', '--shell=9002', '--transport=\"tcp\"', '--iopub=9004', '--f=/home/gabro/.local/share/jupyter/runtime/kernel-v2-2901WgmI7MIVlul5.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "#fa inferenza su una cartella di immagini e ne salva i crop su runs/detect/predict#/crop/defect\n",
    "pathCartellaImm = \"/home/gabro/GrapheDetectProject/immProvaInferenza/\"\n",
    "results = model.predict(pathCartellaImm, save_crop = True')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
